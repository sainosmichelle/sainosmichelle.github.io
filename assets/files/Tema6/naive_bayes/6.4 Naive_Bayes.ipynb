{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6.4 Naive_Bayes.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"L4zQJXYa4IXe"},"source":["#1. Ejercicio de clasificación con algoritmo Naive Bayes\r\n","#Urban Land Cover Data Set\r\n","\r\n","Number of instances 168\r\n","\r\n","Number of Attributes 148\r\n","\r\n","Contiene datos de entrenamiento y prueba para clasificar una imagen aérea de alta resolución en 9 tipos de cobertura de suelo urbano. Para la clasificación se utiliza información espectral, de tamaño, forma y textura de múltiples escalas. \r\n"," El conjunto de datos de prueba proviene de una muestra aleatoria de la imagen.\r\n","\r\n","Las clases de cobertura terrestre son:\r\n","\r\n","\r\n","* Árboles\r\n","* Césped\r\n","* Suelo\r\n","* Concreto\r\n","* Asfalto\r\n","* Edificios\r\n","* Coches\r\n","* Piscinas\r\n","* Sombras\r\n","\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"oJKJFiO4BxWG"},"source":["#2. Importación de librerías y montando google drive"]},{"cell_type":"code","metadata":{"id":"hTDsgpLMlnp-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613183341264,"user_tz":360,"elapsed":671,"user":{"displayName":"Berenice Martinez Tellez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAUnsieqmqd6b2LC0S4JdKtRblPC6wW88O2UbK=s64","userId":"09524167993445699607"}},"outputId":"55f42ca2-8604-4c92-d0fb-8a4970c8272f"},"source":["import ____ as np\n","import matplotlib.pyplot as ____\n","import ____ as pd\n","import seaborn as ____\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q61mYQrXWBhL"},"source":["from sklearn._____ import LabelEncoder\r\n","from sklearn.preprocessing import ____, ____\r\n","from sklearn.feature_selection import SelectKBest\r\n","from sklearn.feature_selection import f_classif\r\n","from sklearn.model_selection import KFold\r\n","from sklearn._____ import confusion_matrix\r\n","from sklearn.model_selection import cross_val_score, cross_val_predict\r\n","from sklearn import metrics\r\n","from collections import Counter\r\n","import itertools"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HW7h8qbhB-dK"},"source":["# 3. Lectura y visualuzación del dataset\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"HY5T8e7G4zX3"},"source":["path = ______\r\n","train_path = os.path.join(path,'training.csv')\r\n","test_path = os.path.join(path,'testing.csv')\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6ugdt9u5Kq3"},"source":["train_df = ______(train_path)\r\n","test_df = pd.read_csv(______)\r\n","clases = train_df['label'].unique()\r\n","train_df.dropna(axis=0, inplace=True)\r\n","train_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5F2z0WzXMcH-"},"source":["##Balance de clases "]},{"cell_type":"code","metadata":{"id":"kxMhRnOCQ2hO"},"source":["print('Número de instancias por clase, conjunto de entrenamiento')\r\n","print(______['label'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Go3A2l2qSULg"},"source":["f,ax=plt.subplots(1,2,figsize=(16,5))\r\n","\r\n","sns.countplot('label',data=train_df,ax=ax[0])\r\n","ax[0].set_title('Distribution of Classes TRAIN')\r\n","\r\n","sns.countplot('label',data=test_df,ax=ax[1])\r\n","ax[1].set_title('Distribution of Classes TEST')\r\n","\r\n","ax[0].set_ylim(0,100)\r\n","ax[1].set_ylim(0,100)\r\n","\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wIGw8YJ1BJlS"},"source":["print ('Conteo de valores faltantes: \\n')\r\n","print (______.isna().sum().sum())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yT3amNLiVSU8"},"source":["#4. Escalamiento"]},{"cell_type":"code","metadata":{"id":"wvXm9xrxVVQh"},"source":["scaler =______()\r\n","train_df.loc[:, train_df.columns != 'label'] = scaler.fit_transform(train_df.loc[:, train_df.columns != 'label'])\r\n","test_df.loc[:, test_df.columns != 'label'] = scaler.transform(test_df.loc[:, test_df.columns != 'label'])\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CRERZhyuThrB"},"source":["#5. Codificación de etiquetas"]},{"cell_type":"code","metadata":{"id":"-IzKaR62Tgga"},"source":["le = ______()\r\n","train_df['label'] = le.______(train_df.label.values)\r\n","test_df['label'] = le.______(test_df.label.values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0wX2sXDonPzm"},"source":["train_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JKvmdslKKAgY"},"source":["## 6. Selección de características por análisis de correlación (Coeficiente de Pearson)"]},{"cell_type":"code","metadata":{"id":"pjZS37Reeqb0"},"source":["#\"todas con todas\"\r\n","correlations = train_df.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index() \r\n","correlations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mq75WcH1es_l"},"source":["#remueve las que son consigo mismas\r\n","correlations = correlations[correlations['level_0'] != correlations['level_1']] \r\n","correlations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v24vtU4Ne31Z"},"source":["#identificar las que tienen correlación mayor a 0.99\r\n","correlations = correlations.loc[correlations[0] >= 0.99]  \r\n","#correlations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oSaqo6LHfQek"},"source":["removable_features = set(list(correlations['level_1']))\r\n","len(removable_features)\r\n","#removable_features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jUi7a05eOc_5"},"source":["Eliminar las removable features a los conjuntos de train y test"]},{"cell_type":"code","metadata":{"id":"_7E_w9zLaiGV"},"source":["train_df = train_df.drop(______, axis=______)\r\n","train_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iX1lMh1Ebz1v"},"source":["test_df = test_df.drop(removable_features, axis=1)\r\n","test_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cp-nOwWA5SP4"},"source":["#7. Modelo de clasificación con Naive Bayes y k-fold CV\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"kb_GqMaLntZB"},"source":["X_train = train_df.values[:,1:]\r\n","Y_train = train_df.values______\r\n","\r\n","X_test = test_df.values______\r\n","Y_test = test_df.values[:,0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d_2SCoOibizQ"},"source":["print(f'Dimension de los datos de entrenamiento {X_train.shape}')\r\n","print(f'Dimension de los datos de prueba {X_test.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VgIIE292DTyn"},"source":["seed = 42\r\n","from sklearn.naive_bayes import GaussianNB \r\n","NB =  GaussianNB()\r\n","kfold = KFold(n_splits=______, random_state=seed, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4CbalklDWOd"},"source":["score_train = cross_val_score(NB, ______, ______, cv=kfold, scoring=\"accuracy\")\r\n","print(\"Train Acc: %s\"%score_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlXKyFuQDgIf"},"source":["y_pred = cross_val_predict(NB, ______, ______, cv=10)\r\n","\r\n","score = metrics.accuracy_score(Y_test, y_pred)\r\n","print(\"Test Acc: %s\"%score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JefupL9l7_MN"},"source":["#Funciones para MC y métricas\r\n","def plot_confusion_matrix(cm, classes, tit, normalize=False):\r\n","    if normalize:\r\n","        cm = cm.astype('float')/cm.sum(axis=1)\r\n","        title, fmt = 'Matriz de confusión normalizada', '.2f'\r\n","    else:\r\n","        title, fmt = tit, 'd'\r\n","    plt.figure(figsize=(10,8))\r\n","    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\r\n","    plt.title(title)#, fontsize=12)\r\n","    plt.colorbar(pad=0.05)\r\n","    tick_marks = np.arange(len(classes))\r\n","    plt.xticks(tick_marks, classes, rotation=40)\r\n","    plt.yticks(tick_marks, classes)\r\n","    thresh = cm.max()/2.\r\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n","        plt.text(j, i, format(cm[i, j], fmt),horizontalalignment=\"center\", \r\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\r\n","    plt.tight_layout()\r\n","    plt.ylabel('Clase Verdadera')#, fontsize=10)\r\n","    plt.xlabel('Clase Predicha')#, fontsize=10)\r\n","    plt.savefig(title+'.png')\r\n","    #plt.grid(b=None)\r\n","    plt.show()\r\n","def sens_spec(cls_names, y_true, y_pred, pesos):\r\n","  sensitivity = []\r\n","  specificity = []\r\n","  acc=[]\r\n","  for i,name in enumerate(cls_names):\r\n","    TP = np.sum((y_true==name) & (y_pred==name))\r\n","    TN = np.sum((y_true!=name) & (y_pred!=name))\r\n","    FP = np.sum((y_true!=name) & (y_pred==name))\r\n","    FN = np.sum((y_true==name) & (y_pred!=name))\r\n","    sensitivity.append(TP/(TP+FN))\r\n","    specificity.append(FP/(TN+FP))\r\n","    acc.append(TP/(TP+FP))\r\n","  sensitivity.append(sum([x*y for x,y in zip(sensitivity,pesos)]))\r\n","  specificity.append(sum([x*y for x,y in zip(specificity,pesos)]))\r\n","  acc.append(sum([x*y for x,y in zip(acc,pesos)]))\r\n","  return sensitivity, specificity, acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5hSE3ZMayj-4"},"source":["Matriz de confusión"]},{"cell_type":"code","metadata":{"id":"gfopJMF6oQyO"},"source":["n_clases = 9\r\n","predictions = np.float32(y_pred)\r\n","true_labels = np.float32(Y_test)\r\n","cnf_matrix = confusion_matrix(true_labels, predictions, labels=range(n_clases))\r\n","tit = 'Matriz de confusión NB'\r\n","plot_confusion_matrix(cnf_matrix,clases, tit, normalize=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CTnf5JSjyxN5"},"source":["Pesos por clase"]},{"cell_type":"code","metadata":{"id":"eThhslkcC0TY"},"source":["pesos = train_df['label'].value_counts().sort_index().tolist()/ np.sum(train_df['label'].value_counts().tolist())\r\n","print(clases)\r\n","print(pesos)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"51-nBGIj64qm"},"source":["sensitivity, specificity, acc = sens_spec(range(9), true_labels, predictions, pesos)\r\n","d = {'Sensitivity':sensitivity, 'Specificity':specificity, 'Accuracy':acc}\r\n","ind = clases.tolist()+['Promedio']\r\n","df = pd.DataFrame(d, index=ind)\r\n","index = df.index\r\n","precprom = sum([x*y for x,y in zip(acc,pesos)])\r\n","index.name = 'Acc Pond: %s'%precprom\r\n","sns.set(rc={'figure.figsize':(11.7,8.27)})\r\n","sns.heatmap(df, annot=True)"],"execution_count":null,"outputs":[]}]}